{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "153f4217-f1b9-4a46-99eb-5c5fb8579d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "080ef8d1-5c7f-4609-a080-0756f9932e2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data cleannnn.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/mary/Downloads/\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# this will be the path to your local directory where the repository is saved\u001b[39;00m\n\u001b[1;32m      3\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata cleannnn.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \n\u001b[0;32m----> 4\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[1;32m      5\u001b[0m data\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data cleannnn.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "os.chdir('/Users/mary/Downloads/data cleannnn.csv') # this will be the path to your local directory where the repository is saved\n",
    "file_path = 'data/df_arabica_clean.csv'  \n",
    "data = pd.read_csv(file_path)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a2b08dc-a87b-413d-838a-c608ac41444e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAroma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFlavor\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAftertaste\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAcidity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBalance\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOverall\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Extract relevant data\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m X \u001b[38;5;241m=\u001b[39m data[features]\u001b[38;5;241m.\u001b[39mdropna()\n\u001b[1;32m      6\u001b[0m X\u001b[38;5;241m.\u001b[39mhead()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Standardize the data\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Select numerical columns for clustering\n",
    "features = ['Aroma', 'Flavor', 'Aftertaste', 'Acidity', 'Body','Balance', 'Overall']\n",
    "\n",
    "# Extract relevant data\n",
    "X = data[features].dropna()\n",
    "X.head()\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cee846-6c15-4a97-b569-7772da41c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Elbow method adn silhouette score to find the optimal number of clusters\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "K = range(2, 11)  # Silhouette score requires at least 2 clusters\n",
    "\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "\n",
    "# Plot Elbow Curve and Silhouette Scores\n",
    "fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.plot(K, inertia, 'bo-', label='Inertia')\n",
    "ax2.plot(K, silhouette_scores, 'r^-', label='Silhouette Score')\n",
    "\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('Inertia', color='b')\n",
    "ax2.set_ylabel('Silhouette Score', color='r')\n",
    "plt.title('Elbow Method and Silhouette Analysis for Optimal k')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d942dd6c-04ef-4e58-a16d-81b1b1fad681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the elbow plot, let's choose k = 3 as the optimal number of clusters trying 4 also\n",
    "# Set the number of clusters for k=3 and k=4\n",
    "kmeans_3 = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_4 = KMeans(n_clusters=4, random_state=42)\n",
    "\n",
    "# Fit the models to the scaled data\n",
    "kmeans_3.fit(X_scaled)\n",
    "kmeans_4.fit(X_scaled)\n",
    "\n",
    "# Get the cluster labels for each k\n",
    "data['Cluster_k3'] = kmeans_3.labels_\n",
    "data['Cluster_k4'] = kmeans_4.labels_\n",
    "\n",
    "# Check the first few rows of the dataframe with both cluster labels\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded82561-ab6f-44e6-96af-2cb71b529034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Function to visualize clusters for a given k\n",
    "def plot_clusters(k, X_scaled):\n",
    "    # Fit KMeans with specified number of clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    # Reduce to 2D for visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Plot clusters\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', marker='o', edgecolor='k', s=50)\n",
    "    plt.title(f'Visualization of Clusters for k = {k}')\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.colorbar(label='Cluster Label')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize clusters for k = 3\n",
    "plot_clusters(3, X_scaled)\n",
    "\n",
    "# Visualize clusters for k = 4\n",
    "plot_clusters(4, X_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa91055-fad4-4d5d-885f-1a19149a5bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the scaled data and add the cluster labels\n",
    "X_df = pd.DataFrame(X, columns=features)  # Assuming 'features' is your list of selected feature names\n",
    "X_df['Cluster_k3'] = kmeans_3.labels_\n",
    "X_df['Cluster_k4'] = kmeans_4.labels_\n",
    "\n",
    "\n",
    "# Display cluster means for k = 3\n",
    "cluster_means_k3 = X_df.groupby('Cluster_k3').mean()\n",
    "print(\"Cluster Means for k = 3:\")\n",
    "print(cluster_means_k3)\n",
    "\n",
    "# Display cluster means for k = 4\n",
    "cluster_means_k4 = X_df.groupby('Cluster_k4').mean()\n",
    "print(\"\\nCluster Means for k = 4:\")\n",
    "print(cluster_means_k4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a96258-5cb7-4b19-9955-a8702e4419e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reduce the scaled data to 2D for plotting\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "def plot_decision_boundary(k, data_pca):\n",
    "    # Generate a mesh grid for the decision boundary\n",
    "    x_min, x_max = data_pca[:, 0].min() - 1, data_pca[:, 0].max() + 1\n",
    "    y_min, y_max = data_pca[:, 1].min() - 1, data_pca[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    # Fit KMeans on the PCA-reduced data\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans.fit(data_pca)\n",
    "    Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    cluster_labels = kmeans.labels_\n",
    "    \n",
    "    # Plot decision boundaries and data points\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, cmap='Pastel2', alpha=0.3)  # Decision boundaries\n",
    "    plt.scatter(data_pca[:, 0], data_pca[:, 1], c=cluster_labels, cmap='viridis', edgecolor='k', s=50)  # Data points\n",
    "    \n",
    "    # Plot centroids\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='X', label='Centroids')\n",
    "    \n",
    "    # Labels and title\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.title(f'Decision Boundaries Between Clusters in PCA-Reduced 2D Space (k = {k})')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for k = 3\n",
    "plot_decision_boundary(3, data_pca)\n",
    "\n",
    "# Plot for k = 4\n",
    "plot_decision_boundary(4, data_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f223d90-c6e6-4471-9ddd-1c699472db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to fit GMM and plot decision boundaries\n",
    "def plot_gmm_decision_boundary(k, data_pca):\n",
    "    # Fit the GMM model\n",
    "    gmm = GaussianMixture(n_components=k, random_state=0)\n",
    "    gmm.fit(data_pca)\n",
    "    labels = gmm.predict(data_pca)\n",
    "    \n",
    "    # Generate a mesh grid for decision boundaries\n",
    "    x_min, x_max = data_pca[:, 0].min() - 1, data_pca[:, 0].max() + 1\n",
    "    y_min, y_max = data_pca[:, 1].min() - 1, data_pca[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    # Predict on the mesh grid for decision boundary visualization\n",
    "    Z = gmm.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    # Plot decision boundaries and data points\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, cmap='Pastel2', alpha=0.3)  # Decision boundary regions\n",
    "    plt.scatter(data_pca[:, 0], data_pca[:, 1], c=labels, cmap='viridis', edgecolor='k', s=50)  # Data points\n",
    "    \n",
    "    # Plot GMM component means (centroids)\n",
    "    centroids = gmm.means_\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='X', label='GMM Centroids')\n",
    "    \n",
    "    # Labels and title\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.title(f'GMM Decision Boundaries in PCA-Reduced 2D Space (k = {k})')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for GMM with k = 3\n",
    "plot_gmm_decision_boundary(3, data_pca)\n",
    "\n",
    "# Plot for GMM with k = 4\n",
    "plot_gmm_decision_boundary(4, data_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5121553a-1cb4-4f58-a6cd-d651d9745167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne_clusters(data, k):\n",
    "    \"\"\"\n",
    "    Plots t-SNE clusters with KMeans centroids and convex hulls.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: numpy array or pandas DataFrame, preprocessed data to be reduced with t-SNE.\n",
    "    - k: int, number of clusters for KMeans.\n",
    "    \"\"\"\n",
    "    # Reduce data to 2D using t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    data_tsne = tsne.fit_transform(data)\n",
    "\n",
    "    # Fit KMeans on the t-SNE-transformed data with specified k\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    cluster_labels = kmeans.fit_predict(data_tsne)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    # Plot t-SNE clusters with centroids\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(data_tsne[:, 0], data_tsne[:, 1], c=cluster_labels, cmap='viridis', edgecolor='k', s=50)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='X', label='Centroids')\n",
    "\n",
    "    # Draw convex hulls around each cluster for boundary-like effect\n",
    "    for i in range(k):\n",
    "        points = data_tsne[cluster_labels == i]\n",
    "        hull = ConvexHull(points)\n",
    "        plt.plot(points[hull.vertices, 0], points[hull.vertices, 1], 'k-', lw=2)\n",
    "\n",
    "    # Labels and title\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.title(f't-SNE Cluster Visualization for k={k} with Convex Hulls')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711c0c73-a089-4b24-9886-ef06c405ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_clusters(data_scaled, k=3)  # For k=3\n",
    "plot_tsne_clusters(data_scaled, k=4)  # For k=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0481bcc2-25cc-43c3-b44f-564560658064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_umap_decision_boundaries(data, k):\n",
    "    \"\"\"\n",
    "    Reduces data to 2D using UMAP, applies KMeans clustering, and plots decision boundaries.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: numpy array or pandas DataFrame, preprocessed data to be reduced with UMAP.\n",
    "    - k: int, number of clusters for KMeans.\n",
    "    \"\"\"\n",
    "    # Step 1: Apply UMAP for 2D dimensionality reduction\n",
    "    umap_reducer = umap.UMAP(n_components=2, random_state=0)\n",
    "    data_umap = umap_reducer.fit_transform(data).astype(np.float64)  # Convert to float64\n",
    "\n",
    "    # Step 2: Fit KMeans on the UMAP-reduced data with specified k\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    cluster_labels = kmeans.fit_predict(data_umap)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    # Step 3: Create a mesh grid for plotting decision boundaries based on UMAP data range\n",
    "    x_min, x_max = data_umap[:, 0].min() - 1, data_umap[:, 0].max() + 1\n",
    "    y_min, y_max = data_umap[:, 1].min() - 1, data_umap[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "    # Ensure mesh grid is in float64 format\n",
    "    mesh_points = np.c_[xx.ravel(), yy.ravel()].astype(np.float64)\n",
    "\n",
    "    # Predict the cluster for each point on the grid\n",
    "    Z = kmeans.predict(mesh_points).reshape(xx.shape)\n",
    "\n",
    "    # Step 4: Plot decision boundaries, clusters, and centroids\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, cmap='Pastel2', alpha=0.3)\n",
    "    plt.scatter(data_umap[:, 0], data_umap[:, 1], c=cluster_labels, cmap='viridis', edgecolor='k', s=50)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], s=300, c='red', marker='X', label='Centroids')\n",
    "\n",
    "    # Label and title\n",
    "    plt.xlabel('UMAP Component 1')\n",
    "    plt.ylabel('UMAP Component 2')\n",
    "    plt.title(f'UMAP-Based Decision Boundaries for k={k}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcaee7c-ce0d-4a84-9c02-bd2626fa5c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_umap_decision_boundaries(data_scaled, k=3)  # For k=3\n",
    "plot_umap_decision_boundaries(data_scaled, k=4)  # For k=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2018a0c-6cbe-473f-8335-737bd934a4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
